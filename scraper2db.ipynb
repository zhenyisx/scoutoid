{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP8yJP6S0Y1c91RX3U2B1ED",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhenyisx/scoutoid/blob/main/scraper2db.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To scrape website and populate to database (csv file) on Google Drive. The files can then be uploaded to Goolge Storage for clients (i.e., stremlit) to consume."
      ],
      "metadata": {
        "id": "Z46myB-0fZNQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import and Function Definitions"
      ],
      "metadata": {
        "id": "Mh_peAjDPRXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import date, datetime, timedelta\n",
        "import re\n",
        "\n",
        "from google.colab import  drive\n",
        "drive.mount('/drive')\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# TODO\n",
        "# - home team and away team are reverse in current table\n",
        "# - team should add city and state"
      ],
      "metadata": {
        "id": "_dEawkCOfhSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_scores_from_soup(maxpreps_day_soup, this_day):\n",
        "  \"\"\"to get scores of games in one day.\n",
        "\n",
        "  Args:\n",
        "      maxpreps_day_soup (soup): a soup parsed from MaxPreps HTML by BS.\n",
        "      this_day (str): the date.\n",
        "\n",
        "  Returns:\n",
        "      schedulf_df (dataframe): the dataframe of all game results.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # create dateframe of game schedules and scores\n",
        "  schedule_df = pd.DataFrame({'Date': pd.Series(dtype='datetime64[ns]'),\n",
        "                    'Home Team': pd.Series(dtype='str'),\n",
        "                    'Home Score': pd.Series(dtype='int'),\n",
        "                    'Away Team': pd.Series(dtype='str'),\n",
        "                    'Away Score': pd.Series(dtype='int')})\n",
        "\n",
        "  # populate the datafrme by analyzing the soup (MaxPreps specific)\n",
        "  # to use ChromeDev Tool to get the following rules of find \n",
        "  matches = maxpreps_day_soup.find('div', {'class': 'contests'})\n",
        "\n",
        "  for m in matches.find_all('ul', {'class': 'teams'}):\n",
        "    raw_record = m.find_all(\"li\")\n",
        "\n",
        "    # home score\n",
        "    try:\n",
        "      home_score = int(raw_record[0].find('div', {'class': 'score'}).text.strip())\n",
        "    except ValueError:\n",
        "        # Handle the exception\n",
        "        print('score not valid, using -1 instead')\n",
        "        home_score = -1\n",
        "    except AttributeError:\n",
        "        # Handle the exception\n",
        "        print('score not valid, using -1 instead')\n",
        "        home_score = -1\n",
        "    # print(home_score)\n",
        "\n",
        "    # home name\n",
        "    home_name = raw_record[0].find('div', {'class': 'name'}).text.strip()\n",
        "    # print(home_name)\n",
        "\n",
        "    # away score\n",
        "    try:\n",
        "      away_score = int(raw_record[1].find('div', {'class': 'score'}).text.strip())\n",
        "    except ValueError:\n",
        "        # Handle the exception\n",
        "        print('score not valid, using -1 instead')\n",
        "        away_score = -1\n",
        "    except AttributeError:\n",
        "        # Handle the exception\n",
        "        print('score not valid, using -1 instead')\n",
        "        away_score = -1\n",
        "    # print(away_score)\n",
        "\n",
        "    # away name\n",
        "    away_name = raw_record[1].find('div', {'class': 'name'}).text.strip()\n",
        "    # print(away_name)\n",
        "\n",
        "    m_result = {'Date':this_day, 'Home Team':home_name, 'Home Score':home_score, \n",
        "                'Away Team':away_name, 'Away Score':away_score}\n",
        "    schedule_df = schedule_df.append(m_result, ignore_index=True)\n",
        "\n",
        "  schedule_df = schedule_df.drop_duplicates(keep='last')\n",
        "\n",
        "  return schedule_df\n",
        "  \n",
        "\n",
        "def get_scores_from_maxpreps_for_one_day(today):\n",
        "  \"\"\"to get scores of games from MaxPreps in one day.\n",
        "\n",
        "  Example:\n",
        "  df = get_scores_from_maxpreps_for_one_day(\"12/1/2022\")\n",
        "  print(df)\n",
        "\n",
        "  Args:\n",
        "      this_day (str): the date str in '%m/%d/%Y' format.\n",
        "\n",
        "  Returns:\n",
        "      schedulf_df (dataframe): the dataframe of all game results.\n",
        "\n",
        "  \"\"\"\n",
        "  # maxpreps's homepage\n",
        "  maxpreps_url = 'https://www.maxpreps.com/ca/central-coast-section/soccer/girls/scores/?date={}'.format(today)  # please change the date if needed\n",
        "\n",
        "  # Use requests to retrieve data from a given URL\n",
        "  maxpreps_response = requests.get(maxpreps_url, headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"})\n",
        "\n",
        "  # Parse the whole HTML page using BeautifulSoup\n",
        "  maxpreps_soup = BeautifulSoup(maxpreps_response.text, 'html.parser')\n",
        "\n",
        "  # Title of the parsed page\n",
        "  print(maxpreps_soup.title.text)\n",
        "\n",
        "  # check if the dates match\n",
        "  try:\n",
        "    content_date = datetime.strptime(maxpreps_soup.title.text.split('|')[1].strip().split()[0], '%m/%d/%Y')\n",
        "    # another way to find the content current date\n",
        "    # content = maxpreps_soup.find('div', {'class': 'calendar'}).find('ol', {'class': 'week'}).find('a', {'class': 'btn btn-default active'})\n",
        "    # content_date = datetime.strptime(content.text, '%m/%d/%Y'))\n",
        "    if content_date == datetime.strptime(today, '%m/%d/%Y'):\n",
        "      print('Found Games for {}'.format(content_date))\n",
        "      scores_df = get_scores_from_soup(maxpreps_soup, content_date)\n",
        "      return scores_df\n",
        "    else:\n",
        "      print('No Games found for {}'.format(today))\n",
        "  except TypeError:\n",
        "    print('No Games found for {}'.format(today))\n",
        "  except ValueError:\n",
        "    print('No Games found for {}'.format(today))\n",
        "\n",
        "\n",
        "def get_scores_from_maxpreps_for_range(start_date, end_date):\n",
        "  \"\"\"to get scores of games from MaxPreps in a date range.\n",
        "\n",
        "  Example:\n",
        "  start_date = date(2022, 11, 10)\n",
        "  end_date = date(2023, 2, 10)\n",
        "  scores = get_scores_from_maxpreps_for_range(start_date, end_date)\n",
        "\n",
        "  Args:\n",
        "      start_date (date): the start date.\n",
        "      end_date (date): the end date.\n",
        "\n",
        "  Returns:\n",
        "      schedulf_df (dataframe): the dataframe of all game results.\n",
        "\n",
        "  \"\"\"\n",
        "  def daterange(start_date, end_date):\n",
        "    for n in range(int((end_date - start_date).days)):\n",
        "        yield start_date + timedelta(n)\n",
        "\n",
        "  dfs = []\n",
        "  for single_date in daterange(start_date, end_date):\n",
        "      today = single_date.strftime(\"%m/%d/%Y\")\n",
        "      scores_df = get_scores_from_maxpreps_for_one_day(today)\n",
        "      if scores_df is None:\n",
        "        print('No games found on {}'.format(today))\n",
        "      else:\n",
        "        print('Found {} games for {}'.format(len(scores_df), today))\n",
        "      dfs.append(scores_df)\n",
        "\n",
        "  # see pd.concat documentation for more info\n",
        "  concat_scores_df = pd.concat(dfs)\n",
        "  concat_scores_df = concat_scores_df.drop_duplicates(keep='last')\n",
        "  return concat_scores_df\n"
      ],
      "metadata": {
        "id": "XG32BV0UzUii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rankings_from_scores(schedule_df):\n",
        "  \"\"\"to ranking statistics of teams from the schedules and scores\n",
        "\n",
        "  Example:\n",
        "  df = get_scores_from_maxpreps_for_one_day(\"12/1/2022\")\n",
        "  print(df)\n",
        "\n",
        "  Args:\n",
        "      schedule_df (dataframe): the scores dataframe.\n",
        "\n",
        "  Returns:\n",
        "      ranking_df (dataframe): the dataframe of ranking statistics.\n",
        "\n",
        "  \"\"\"\n",
        "  teams = list(set(list(schedule_df[\"Home Team\"].unique()) + \n",
        "                   list(schedule_df[\"Away Team\"].unique())))\n",
        "\n",
        "  # initialize the ranking dataframe\n",
        "  ranking_df = pd.DataFrame({\n",
        "                    'Team': pd.Series(dtype='str'),\n",
        "                    'Total Points': pd.Series(dtype='int'),\n",
        "                    'Total Games': pd.Series(dtype='int'),\n",
        "                    'Total Wins': pd.Series(dtype='int'),\n",
        "                    'Total Losses': pd.Series(dtype='int'),\n",
        "                    'Total Ties': pd.Series(dtype='int'),\n",
        "                    'PPG': pd.Series(dtype='float'),})\n",
        "\n",
        "  for t in teams:\n",
        "    t_result = {'Team':t, 'Total Points': 0,\n",
        "                    'Total Games': 0,\n",
        "                    'Total Wins': 0,\n",
        "                    'Total Losses': 0,\n",
        "                    'Total Ties': 0,\n",
        "                    'PPG':0}\n",
        "    ranking_df = ranking_df.append(t_result, ignore_index=True)\n",
        "\n",
        "  # populate ranking dataframe based on schedule\n",
        "  for index,row in schedule_df.iterrows():\n",
        "    home_team = row['Home Team']\n",
        "    away_team = row['Away Team']\n",
        "    home_score = row['Home Score']\n",
        "    away_score = row['Away Score']\n",
        "\n",
        "    ranking_df.loc[(ranking_df['Team'] == home_team), 'Total Games'] += 1\n",
        "    ranking_df.loc[(ranking_df['Team'] == away_team), 'Total Games'] += 1\n",
        "\n",
        "    if home_score > away_score: # home win  \n",
        "      ranking_df.loc[(ranking_df['Team'] == home_team), 'Total Wins'] += 1\n",
        "      ranking_df.loc[(ranking_df['Team'] == home_team), 'Total Points'] += 3\n",
        "      ranking_df.loc[(ranking_df['Team'] == away_team), 'Total Losses'] += 1\n",
        "    elif home_score < away_score: # away win\n",
        "      ranking_df.loc[(ranking_df['Team'] == away_team), 'Total Wins'] += 1\n",
        "      ranking_df.loc[(ranking_df['Team'] == away_team), 'Total Points'] += 3\n",
        "      ranking_df.loc[(ranking_df['Team'] == home_team), 'Total Losses'] += 1\n",
        "    elif home_score == away_score: # tie\n",
        "      ranking_df.loc[(ranking_df['Team'] == home_team), 'Total Ties'] += 1\n",
        "      ranking_df.loc[(ranking_df['Team'] == away_team), 'Total Ties'] += 1\n",
        "      ranking_df.loc[(ranking_df['Team'] == home_team), 'Total Points'] += 1\n",
        "      ranking_df.loc[(ranking_df['Team'] == away_team), 'Total Points'] += 1\n",
        "\n",
        "  ranking_df['PPG'] = ranking_df['Total Points'] / ranking_df['Total Games']\n",
        "\n",
        "  ranking_df = ranking_df.sort_values(by=['PPG', 'Total Points'], \n",
        "                                      ascending=False)\n",
        "  ranking_df = ranking_df.drop_duplicates(keep='last')\n",
        "  return ranking_df\n"
      ],
      "metadata": {
        "id": "nksiVfx8w3O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running Script"
      ],
      "metadata": {
        "id": "qGqG335tPH0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# demo\n",
        "df = get_scores_from_maxpreps_for_one_day(\"2/21/2023\")\n",
        "print(df)"
      ],
      "metadata": {
        "id": "vVlhCgezUxXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get all scores\n",
        "# please note game info of previous seasons/years are not accessible,\n",
        "# only current season is accessible.\n",
        "\n",
        "# load historical data\n",
        "# scores.csv was generated between 11/10/2022 and 2/1/2023\n",
        "hist_scores = pd.read_csv('/drive/My Drive/scores.csv')\n",
        "\n",
        "start_date = date(2023, 2, 1)\n",
        "end_date = date(2023, 3, 12)\n",
        "new_scores = get_scores_from_maxpreps_for_range(start_date, end_date)\n",
        "\n",
        "scores = pd.concat([hist_scores, new_scores])\n",
        "scores['Date'] = pd.to_datetime(scores['Date'])\n",
        "scores = scores.drop_duplicates(keep='last')\n",
        "scores.to_csv('/drive/My Drive/scores_2223_03112023.csv', index=False)"
      ],
      "metadata": {
        "id": "GuJkF8RrOaut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get all rankings\n",
        "rankings = get_rankings_from_scores(scores)\n",
        "rankings.to_csv('/drive/My Drive/rankings_2223_03112023.csv', index=False)"
      ],
      "metadata": {
        "id": "sah_im9Sr2a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "CTCxzxmrjPkz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Method"
      ],
      "metadata": {
        "id": "fydMxgKcV-ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new dataset for the scores and rankings\n",
        "# one shot (not combining)\n",
        "\n",
        "# combine with historical data"
      ],
      "metadata": {
        "id": "SDDWC6P9WClY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test of Script"
      ],
      "metadata": {
        "id": "IR1OlpxXM9_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### analyze game summary page"
      ],
      "metadata": {
        "id": "SLbKiODcAHzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "single_date =  date(2023, 2, 28)\n",
        "today = single_date.strftime(\"%m/%d/%Y\")\n",
        "\n",
        "# maxpreps's homepage\n",
        "maxpreps_url = 'https://www.maxpreps.com/ca/central-coast-section/soccer/girls/scores/?date={}'.format(today)  # please change the date if needed\n",
        "\n",
        "# Use requests to retrieve data from a given URL\n",
        "maxpreps_response = requests.get(maxpreps_url, headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"})\n",
        "\n",
        "# Parse the whole HTML page using BeautifulSoup\n",
        "maxpreps_soup = BeautifulSoup(maxpreps_response.text, 'html.parser')\n"
      ],
      "metadata": {
        "id": "biTur2QgM7iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find all the div (each div is a game) and print a link in each div\n",
        "for gamediv in maxpreps_soup.find_all(\"div\", {\"class\": \"contest-box-item\"}):#maxpreps_soup.findAll('a', {'class': 'c-c'}):\n",
        "    try:\n",
        "        # print(gamediv['href'])\n",
        "        # print(gamediv)\n",
        "        print(gamediv.find('a')['href'])\n",
        "    except KeyError:\n",
        "        print(\"no href\")\n",
        "        pass"
      ],
      "metadata": {
        "id": "jSV0LmfhSSF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find  details of one game using the link\n",
        "# game_detail_page = \"https://www.maxpreps.com/games/2-28-2023/girls-soccer-winter-22-23/presentation-vs-windsor.htm?c=kWgQYflpLk60D3-K60z9pA\" # simple example\n",
        "game_detail_page = \"https://www.maxpreps.com/games/2-22-2023/girls-soccer-winter-22-23/menlo-atherton-vs-mountain-view.htm?c=JPH2i5aDH0yYvdk1bipRfg\" # complete example\n",
        "\n",
        "# Use requests to retrieve data from a given URL\n",
        "gamedetail_response = requests.get(game_detail_page, headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"})\n",
        "\n",
        "# Parse the whole HTML page using BeautifulSoup\n",
        "gamedetail_soup = BeautifulSoup(gamedetail_response.text, 'html.parser')"
      ],
      "metadata": {
        "id": "QmTxCOCFCYp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print('Classes of each table:')\n",
        "# for table in gamedetail_soup.find_all('table'):\n",
        "#     print(table.get('class'))\n",
        "\n",
        "def get_text(ele):\n",
        "  return ele.text if ele is not None else None\n",
        "\n",
        "\n",
        "def find_city(team_name, summary):\n",
        "  \"\"\" find city of team from summary\n",
        "  \"\"\"\n",
        "  res = re.search(r'{} \\((.*?)\\)'.format(team_name), summary, re.IGNORECASE)\n",
        "  if res:\n",
        "      return res.group(1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# need to record howm/away information in one row"
      ],
      "metadata": {
        "id": "iDTaFPw_SuoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create a dataframe from statistics page"
      ],
      "metadata": {
        "id": "ebe4CSAyDjGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find info for a game from its details url\n",
        "def extract_game_info_from_details_page(page_url):\n",
        "  # Use requests to retrieve data from a given URL\n",
        "  page_response = requests.get(page_url, \n",
        "                               headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"})\n",
        "\n",
        "  # Parse the whole HTML page using BeautifulSoup\n",
        "  page_soup = BeautifulSoup(page_response.text, 'html.parser')\n",
        "\n",
        "  # find summary\n",
        "  game_summary = get_text(page_soup.find('p', attrs={'class' : 'contest-description'}))\n",
        "  # print(game_summary)\n",
        "\n",
        "  # find school names (names in table are not reliable)\n",
        "  school_names = [i.text for i in page_soup.find('div', attrs={'class' : 'school-names'}).find_all('a')]\n",
        "  # print(school_names)\n",
        "\n",
        "  # analyze target table\n",
        "  table = page_soup.find('div' , {\"data-l-s-c\":\"box-score\"}).find('table', class_='mx-grid boxscore d-b-s post soccer')\n",
        "  # print(table)\n",
        "\n",
        "  team_names = []\n",
        "  team_cities = []\n",
        "  first_half_scores = []\n",
        "  second_half_scores = []\n",
        "  total_scores = []\n",
        "  shootout_scores = []\n",
        "  is_winner = []\n",
        "\n",
        "  try: \n",
        "    for r in table.tbody.find_all('tr'):\n",
        "      # find team name\n",
        "      team_name = get_text(r.find('th', class_='team first'))\n",
        "      team_names.append(team_name)\n",
        "      # team_cities.append(find_city(team_name, game_summary))\n",
        "      first_half_scores.append(get_text(r.find('td', class_='firsthalf score dw')))\n",
        "      second_half_scores.append(get_text(r.find('td', class_='secondhalf score dw')))\n",
        "      total_scores.append(get_text(r.find('td', class_='score total score')))\n",
        "      shootout_scores.append(get_text(r.find('td', class_='shootout stat tiebreaker dw')))\n",
        "      is_winner.append(get_text(r.find('td', class_='winner last')))  \n",
        "  except AttributeError as err:\n",
        "    print(err)\n",
        "    pass\n",
        "\n",
        "  # team_names = [j if i != j else i for i, j in zip(team_names, school_names)]\n",
        "  team_names = school_names\n",
        "  team_cities = [find_city(j, game_summary) for j in team_names]\n",
        "  return team_names, team_cities, first_half_scores, second_half_scores, total_scores, shootout_scores, is_winner, game_summary\n",
        "\n",
        "game_detail_page1 = \"https://www.maxpreps.com/games/2-28-2023/girls-soccer-winter-22-23/presentation-vs-windsor.htm?c=kWgQYflpLk60D3-K60z9pA\" # simple example\n",
        "game_detail_page2 = \"https://www.maxpreps.com/games/2-22-2023/girls-soccer-winter-22-23/menlo-atherton-vs-mountain-view.htm?c=JPH2i5aDH0yYvdk1bipRfg\" # complete example with shootout\n",
        "game_detail_page3 = \"https://www.maxpreps.com/games/3-2-2023/girls-soccer-winter-22-23/hollister-vs-stone-ridge-christian.htm?c=tv8S33dtGkSS-2ZuVO0vLg\"\n",
        "game_detail_page4 = \"https://www.maxpreps.com/games/11-29-2022/girls-soccer-winter-22-23/gateway-vs-oceana.htm?c=70Hrb_EAd0mWPDTpdhiXoQ\" # example of missing scores\n",
        "game_detail_page5 = \"https://www.maxpreps.com/games/1-6-2023/girls-soccer-winter-22-23/king-city-vs-st-francis.htm?c=ImFn1XMo2EGTXl-9PYtm0A\" # no game result is reported\n",
        "\n",
        "# print(extract_game_info_from_details_page(game_detail_page1))\n",
        "# print(extract_game_info_from_details_page(game_detail_page2))\n",
        "print(extract_game_info_from_details_page(game_detail_page3))\n",
        "print(extract_game_info_from_details_page(game_detail_page5))"
      ],
      "metadata": {
        "id": "_47PmRpnF50l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract team info from school profile page\n",
        "def extract_team_info_from_profile_page(school_profile_url):\n",
        "  \"\"\"\n",
        "  return school name, address, mascot, color, school type, athletic director, phone\n",
        "  \"\"\"\n",
        "  # Use requests to retrieve data from a given URL\n",
        "  page_response = requests.get(school_profile_url, \n",
        "                               headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"})\n",
        "\n",
        "  # Parse the whole HTML page using BeautifulSoup\n",
        "  page_soup = BeautifulSoup(page_response.text, 'html.parser')\n",
        "\n",
        "  school_name = page_soup.find(\"h1\").string\n",
        "\n",
        "  dl_data = page_soup.find_all(\"dd\")\n",
        "  info_list = [dl.text for dl in dl_data]\n",
        "  \n",
        "  return tuple([school_name] + info_list)\n",
        "team_profile_page1 = \"https://www.maxpreps.com/ca/mountain-view/mountain-view-spartans\"\n",
        "\n",
        "print(extract_team_info_from_profile_page(team_profile_page1))\n",
        "\n"
      ],
      "metadata": {
        "id": "LiFdPCtwt_iK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find team profile page url from game page url\n",
        "def find_team_profile_pages(game_page_url):\n",
        "  # Use requests to retrieve data from a given URL\n",
        "  page_response = requests.get(game_page_url, \n",
        "                               headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"})\n",
        "\n",
        "  # Parse the whole HTML page using BeautifulSoup\n",
        "  page_soup = BeautifulSoup(page_response.text, 'html.parser')\n",
        "\n",
        "  page_urls = []\n",
        "  for sdiv in page_soup.find_all(\"div\", {\"class\": \"school-names\"}):\n",
        "    for l in sdiv.find_all('a'):\n",
        "      page_urls.append(\"https://www.maxpreps.com\"+l['href'])\n",
        "  return page_urls\n",
        "\n",
        "game_detail_page5 = \"https://www.maxpreps.com/games/1-6-2023/girls-soccer-winter-22-23/king-city-vs-st-francis.htm?c=ImFn1XMo2EGTXl-9PYtm0A\" # no game result is reported\n",
        "print(find_team_profile_pages(game_detail_page5))"
      ],
      "metadata": {
        "id": "fFZz0IqC4mNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# create dataframe of teams\n",
        "\n",
        "teams_df = pd.DataFrame({'School Name': pd.Series(dtype='str'),\n",
        "                         'School Alias': pd.Series(dtype='str'),\n",
        "                        'School Address': pd.Series(dtype='str'),\n",
        "                        'Mascot': pd.Series(dtype='int'),\n",
        "                        'School Link': pd.Series(dtype='str'),\n",
        "                        'Team Link': pd.Series(dtype='str')})\n",
        "\n",
        "# load schedule df to \n",
        "# step 1: find team link from each game details page\n",
        "# step 2: add a record of team info\n",
        "# step 3: dedupe \n",
        "# note: moving forward we don't need to parse school info from game details page\n",
        "\n",
        "hist_scores_df = pd.read_csv('/drive/My Drive/scores_2223_03182023.csv')\n",
        "for index, row in tqdm(hist_scores_df.iterrows(), total=hist_scores_df.shape[0]):\n",
        "    game_url = row['Web Link']\n",
        "    team_profile_urls = find_team_profile_pages(game_url)\n",
        "    for u in team_profile_urls:\n",
        "        school_profile_url = u.removesuffix(\"soccer/girls/winter/schedule/\")\n",
        "        info_list = extract_team_info_from_profile_page(school_profile_url)\n",
        "        try:\n",
        "          # pd.concat([new_row,df.loc[:]]).reset_index(drop=True)\n",
        "          m_result = pd.DataFrame({'School Name':info_list[0], \n",
        "                      'School Alias':\"\", \n",
        "                      'School Address':info_list[1], \n",
        "                      'Mascot':info_list[2], \n",
        "                      'School Link':school_profile_url,\n",
        "                      'Team Link':u }, index=[0])\n",
        "          teams_df = pd.concat([m_result, teams_df.loc[:]]).reset_index(drop=True)\n",
        "          # teams_df = teams_df.append(m_result, ignore_index=True)\n",
        "        except IndexError as err:\n",
        "          print(err)\n",
        "          print(school_profile_url)\n",
        "          print(u)\n",
        "\n",
        "teams_df = teams_df.drop_duplicates(keep='last')\n",
        "# teams_df.to_csv('/drive/My Drive/teams_2223_03182023.csv', index=False)\n",
        "print(teams_df)\n"
      ],
      "metadata": {
        "id": "drhgrzNP0aFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dateframe of game information\n",
        "def create_game_info_dataframe_from_stats_page(page_url, today):\n",
        "  \"\"\" create game info dataframe from a stats page\n",
        "\n",
        "  today: Date object\n",
        "  \n",
        "  note: details page has no date\n",
        "  return dataframe\n",
        "\n",
        "  \"\"\"\n",
        "  \n",
        "  \n",
        "  # Use requests to retrieve data from a given URL\n",
        "  page_response = requests.get(page_url, headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"})\n",
        "\n",
        "  # Parse the whole HTML page using BeautifulSoup\n",
        "  page_soup = BeautifulSoup(page_response.text, 'html.parser')\n",
        "\n",
        "  # verify the date\n",
        "  title_date = datetime.strptime(page_soup.title.text.split('|')[1].strip().split()[0], '%m/%d/%Y').date()\n",
        "  # print(page_soup.title.text.split('|')[1].strip().split()[0])\n",
        "  # print(title_date)\n",
        "  # print(today)\n",
        "\n",
        "  if title_date == today:\n",
        "      print('Found Games for {}'.format(title_date))\n",
        "      schedule_df = pd.DataFrame({'Date': pd.Series(dtype='datetime64[ns]'),\n",
        "                                'Home Team': pd.Series(dtype='str'),\n",
        "                                'Home City': pd.Series(dtype='str'),\n",
        "                                'Home Score': pd.Series(dtype='int'),\n",
        "                                'Away Team': pd.Series(dtype='str'),\n",
        "                                'Away City': pd.Series(dtype='str'),\n",
        "                                'Away Score': pd.Series(dtype='int'),\n",
        "                                'Game Summary': pd.Series(dtype='str'),\n",
        "                                'Web Link': pd.Series(dtype='str')})\n",
        "      # find all the div (each div is a game) and print a link in each div\n",
        "      for gamediv in page_soup.find_all(\"div\", {\"class\": \"contest-box-item\"}):#maxpreps_soup.findAll('a', {'class': 'c-c'}):\n",
        "          try:\n",
        "              game_detail_url = gamediv.find('a')['href']\n",
        "              print(game_detail_url)\n",
        "              team_names, team_cities, first_half_scores, second_half_scores, total_scores, shootout_scores, is_winner, game_summary = extract_game_info_from_details_page(game_detail_url)\n",
        "              if len(total_scores)==2:\n",
        "                  if shootout_scores[0] == None or shootout_scores[1]==None:\n",
        "                    m_result = {'Date':today, \n",
        "                                'Home Team':team_names[1], \n",
        "                                'Home City':team_cities[1], \n",
        "                                'Home Score':total_scores[1], \n",
        "                                'Away Team':team_names[0],\n",
        "                                'Away City':team_cities[0], \n",
        "                                'Away Score':total_scores[0],\n",
        "                                'Game Summary': game_summary,\n",
        "                                'Web Link': game_detail_url}\n",
        "                  else:\n",
        "                    m_result = {'Date':today, \n",
        "                                'Home Team':team_names[1], \n",
        "                                'Home City':team_cities[1], \n",
        "                                'Home Score':shootout_scores[1], \n",
        "                                'Away Team':team_names[0],\n",
        "                                'Away City':team_cities[0], \n",
        "                                'Away Score':shootout_scores[0],\n",
        "                                'Game Summary': game_summary,\n",
        "                                'Web Link': game_detail_url}\n",
        "                  schedule_df = schedule_df.append(m_result, ignore_index=True)\n",
        "          except (KeyError, TypeError) as err:\n",
        "              print(err)\n",
        "              pass\n",
        "\n",
        "      return schedule_df\n",
        "  else:\n",
        "      print('No Games for {}'.format(today))\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "BD8cT1NnDv-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "today = date(2022, 11, 29)\n",
        "maxpreps_url = 'https://www.maxpreps.com/ca/central-coast-section/soccer/girls/scores/?date={}'.format(today.strftime(\"%m/%d/%Y\")) \n",
        "print(maxpreps_url)\n",
        "\n",
        "\n",
        "print(create_game_info_dataframe_from_stats_page(maxpreps_url, today))"
      ],
      "metadata": {
        "id": "cjE07j2bYLbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reprocess all season data\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "start_date = date(2022, 11, 1)\n",
        "end_date = date(2023, 3, 18)\n",
        "\n",
        "def daterange(start_date, end_date):\n",
        "    for n in range(int((end_date - start_date).days)):\n",
        "        yield start_date + timedelta(n)\n",
        "\n",
        "dfs = []\n",
        "for today in tqdm(daterange(start_date, end_date)):\n",
        "    # print('https://www.maxpreps.com/ca/central-coast-section/soccer/girls/scores/?date={}'.format(today.strftime(\"%m/%d/%Y\")) )\n",
        "    scores_df = create_game_info_dataframe_from_stats_page(\n",
        "        'https://www.maxpreps.com/ca/central-coast-section/soccer/girls/scores/?date={}'.format(today.strftime(\"%m/%d/%Y\")) ,\n",
        "        today)\n",
        "    if scores_df is not None:\n",
        "      print('Found {} games for {}'.format(len(scores_df), today))\n",
        "    dfs.append(scores_df)\n",
        "\n",
        "# see pd.concat documentation for more info\n",
        "if len(dfs)>0:\n",
        "  scores_df = pd.concat(dfs)\n",
        "  scores_df = scores_df.drop_duplicates(keep='last')\n",
        "  print(scores_df)\n",
        "\n",
        "\n",
        "# scores['Date'] = pd.to_datetime(scores['Date'])\n",
        "# scores = scores.drop_duplicates(keep='last')\n",
        "scores_df.to_csv('/drive/My Drive/scores_2223_03182023.csv', index=False)"
      ],
      "metadata": {
        "id": "9GZm39OsYVtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get all rankings\n",
        "rankings = get_rankings_from_scores(scores_df)\n",
        "rankings.to_csv('/drive/My Drive/rankings_2223_03182023.csv', index=False)"
      ],
      "metadata": {
        "id": "h_cUwpCNjVvT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}